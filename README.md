

# Detecci√≥n de puntos de cambio en diversas trayectorias

## Men√∫ de navegaci√≥n
1. [Introducci√≥n](#introducci√≥n)
2. [Estado del arte](#estado-del-arte)
3. [Evaluaci√≥n de M√©todos](#evaluaci√≥n-de-M√©todos)
   - [F1-Score](#f1-score)
   - [Distancia de Hausdorff](#distancia-de-hausdorff)
   - [√çndice de Rand](#√≠ndice-de-rand)
4. [Funciones de costo](#funciones-de-costo)
   - [Funci√≥n de Costo L1](#funci√≥n-de-costo-l1)
   - [Funci√≥n de Costo L2](#funci√≥n-de-costo-l2)
   - [Funci√≥n de Costo Normal](#funci√≥n-de-costo-normal)
   - [Cambio de media con kernel (CostRbf)](#cambio-de-media-con-kernel-costrbf)
   - [Cambio de media con kernel (CostCosine)](#cambio-de-media-con-kernel-costcosine)
   - [Cambio en modelo lineal (CostLinear)](#cambio-en-modelo-lineal-costlinear)
   - [Cambio lineal continuo (CostCLinear)](#cambio-lineal-continuo-costclinearcosto)
   - [Funci√≥n de costo basada en rangos (CostRank)](#funci√≥n-de-costo-basada-en-rangos-costrank)
   - [Detecci√≥n de cambios con m√©trica Mahalanobis (CostMl)](#detecci√≥n-de-cambios-con-m√©trica-mahalanobis-costml)
   - [Cambio de modelo autorregresivo (CostAR)](#cambio-de-modelo-autorregresivo-costar)
5. [M√©todos de b√∫squeda de puntos de cambio](#m√©todos-de-b√∫squeda-de-puntos-de-cambio)
   - [Segmentaci√≥n Binaria (BinSeg)](#segmentaci√≥n-binaria-binseg)
   - [Segmentaci√≥n PELT](#segmentaci√≥n-pelt)
   - [Segmentaci√≥n Bottom-Up](#segmentaci√≥n-bottom-up)
   - [Detecci√≥n Window-Based](#detecci√≥n-window-based)



---
### Introducci√≥n

La detecci√≥n de puntos de cambio consiste en identificar momentos en el tiempo donde las propiedades estad√≠sticas de una serie de datos presentan variaciones. Esta t√©cnica es clave en distintos √°mbitos, como las finanzas, el monitoreo ambiental y el control de calidad, ya que entender los cambios en las tendencias de los datos permite mejorar la toma de decisiones y el an√°lisis predictivo. Detectar estos cambios ayuda a los analistas a reaccionar de forma anticipada ante patrones o anomal√≠as que puedan surgir en los datos.
Lo importante de detectar puntos de cambio es que ayuda a entender mejor los datos y tomar decisiones acertadas. Por ejemplo, en el clima, una variaci√≥n repentina en la temperatura o la presi√≥n puede anticipar tormentas o cambios bruscos en el tiempo. En la salud, notar alteraciones en los signos vitales de un paciente puede servir para prevenir problemas graves. En general, identificar estos cambios a tiempo permite reaccionar r√°pido y adaptarse mejor. 
Existen varios m√©todos para detectar puntos de cambio, como las pruebas estad√≠sticas, que comparan los datos antes y despu√©s del cambio, o los algoritmos de aprendizaje autom√°tico, que encuentran patrones en datos m√°s complejos. Tambi√©n se utilizan enfoques bayesianos, que ajustan la detecci√≥n a medida que se recibe nueva informaci√≥n. Cada uno de estos m√©todos tiene sus ventajas dependiendo del tipo de datos y el objetivo del an√°lisis.
Aunque la detecci√≥n de puntos de cambio tiene muchas ventajas, tambi√©n presenta varios desaf√≠os. Uno de los problemas m√°s comunes es elegir el m√©todo adecuado, porque cada t√©cnica puede dar resultados distintos dependiendo del tipo de datos que tengas. Adem√°s, el ruido y los valores at√≠picos pueden complicar el proceso, provocando falsos positivos o incluso dejando cambios sin detectar. Por eso, es importante que los profesionales tomen en cuenta las caracter√≠sticas de sus datos al aplicar estos m√©todos.
Hay varias herramientas y programas que puedes usar para detectar puntos de cambio en los datos. Lenguajes como Python y R tienen bibliotecas especiales para esto. Por ejemplo, en Python puedes usar la biblioteca ruptures, que te permite aplicar diferentes algoritmos para detectar cambios, visualizar los resultados y ajustar los par√°metros seg√∫n lo necesites. En R, existe el paquete changepoint, que tambi√©n facilita este tipo de an√°lisis. 

---

[Retour au menu](#Men√∫-de-navegaci√≥n)

---

### Estado del arte

El problema de los puntos de cambio ha sido y sigue siendo un tema importante en el estudio de trayectorias, ya que permite identificar momentos clave en los que un proceso cambia. Los primeros trabajos sobre este tema fueron de Page en 1954 y 1955, quien present√≥ esquemas de inspecci√≥n continua para detectar cambios en los datos. En 1955, tambi√©n propuso una prueba estad√≠stica para identificar cambios en par√°metros sin conocer el punto exacto. M√°s tarde, Chernoff y Zacks (1964) utilizaron un enfoque bayesiano para estimar la media de una distribuci√≥n normal que cambia con el tiempo. M√°s recientemente, Gichuhi, Franke y Weizsacker (2008) utilizaron redes neuronales para detectar puntos de cambio en datos binarios, lo que abri√≥ nuevas posibilidades con el aprendizaje autom√°tico.
Muchos investigadores tambi√©n han trabajado en los problemas relacionados con los m√∫ltiples puntos de cambio, entre ellos. Bai y Perron (1998) propusieron m√©todos para estimar y probar modelos con m√∫ltiples cambios estructurales. Pan y Chen (2006) usaron un criterio de informaci√≥n modificado para detectar varios puntos de cambio en los datos. Por su parte, Yao (1984) plante√≥ un enfoque bayesiano para estimar funciones escal√≥n ruidosas, mientras que Barry y Hartigan (1992) introdujeron modelos de partici√≥n para manejar los puntos de cambio. Lee (1998) trabaj√≥ en estimar el n√∫mero de puntos de cambio utilizando un enfoque bayesiano. M√°s recientemente, Lavielle (1999) se centr√≥ en detectar m√∫ltiples puntos de cambio en secuencias de variables dependientes, y Lai, Liu y Xing (2005) analizaron modelos autorregresivos con volatilidad constante para tratar los cambios en los par√°metros de los datos.

Uno de los m√°s conocidos es la segmentaci√≥n binaria, que es bastante popular en este tipo de estudios. Es un m√©todo aproximado con una complejidad computacional de \( O(n \log n) \), donde \( n \) es el n√∫mero de puntos de datos. Por otro lado, el algoritmo de vecindad de segmentos (Scott y Knott, 1974) es m√°s preciso, ya que busca de manera exacta en todo el espacio de segmentaci√≥n, pero su costo computacional es mucho mayor, \( O(Qn^2) \), donde \( Q \) es el n√∫mero m√°ximo de puntos de cambio y \( n \) es el n√∫mero de datos. Si el n√∫mero de puntos de cambio crece linealmente con el aumento de los datos, el costo puede llegar a ser \( O(n^3) \), lo que lo hace menos eficiente a medida que se acumulan m√°s datos.

El m√©todo de partici√≥n √≥ptima (Auger y Lawrence, 1989) es otra opci√≥n, que mejora la eficiencia de la vecindad de segmentos, pero sigue sin igualar la rapidez de la segmentaci√≥n binaria. A pesar de esto, es un enfoque exacto, con una complejidad computacional de \( O(n^2) \), y se aplica en un tipo m√°s reducido de problemas. 
El m√©todo PELT (Pruned Exact Linear Time) es una t√©cnica bastante eficiente para detectar m√∫ltiples puntos de cambio en series temporales. En el estudio de Killick, Eckley y Jonathan (2011), este m√©todo se aplic√≥ a series de datos oce√°nicos y mostr√≥ ser muy r√°pido en comparaci√≥n con otras t√©cnicas, ya que puede manejar grandes vol√∫menes de datos en un tiempo relativamente corto. La clave de su eficiencia est√° en que PELT realiza una poda en el proceso de b√∫squeda, lo que significa que no tiene que revisar todas las posibles divisiones de los datos, sino solo las m√°s relevantes, haciendo que funcione en un tiempo O(n). Por otro lado, en el trabajo de Madon y Hingrat (2014), PELT se utiliz√≥ en combinaci√≥n con un √°rbol de clasificaci√≥n para analizar el comportamiento de los animales. Este enfoque ayud√≥ a detectar cambios significativos en sus movimientos de manera r√°pida y precisa, lo que es especialmente √∫til cuando se tiene que manejar una gran cantidad de datos sobre los patrones de movimiento de los animales.

---

[Retour au menu](#Men√∫-de-navegaci√≥n)

---

### Evaluaci√≥n


Los m√©todos para detectar puntos de cambio se pueden evaluar de dos formas: una es demostrando ciertas propiedades matem√°ticas de los algoritmos y la otra es hacerlo de forma emp√≠rica, calculando distintas m√©tricas.

En lo que sigue, al conjunto de los puntos de cambio verdaderos lo denoto como T* = { t1*, ..., tK* }, y al conjunto de los puntos de cambio estimados lo denoto como TÃÇ = { tÃÇ1, ..., tÃÇKÃÇ }.

#### F1-Score

La m√©trica F1-Score emerge como indicador robusto para evaluar el rendimiento en esta tarea. Su c√°lculo se basa en dos componentes esenciales:

**Precisi√≥n (Prec)**: Mide la fiabilidad de las detecciones  

$$
\text{Prec} = \frac{\text{Detecciones correctas}}{\text{Total de detecciones}} = \frac{|\text{Tp}|}{\hat{K}}
$$

**Exhaustividad (Rec)**: Eval√∫a la capacidad de descubrimiento

$$
\text{Rec} = \frac{\text{Detecciones correctas}}{\text{Total de puntos reales}} = \frac{|\text{Tp}|}{K^*}
$$

Considero que una detecci√≥n es v√°lida cuando existe coincidencia dentro de un margen $M$ muestral:

```math
\text{Tp} = \{ t^* \in \mathcal{T}^* \mid \exists\, \hat{t} \in \widehat{\mathcal{T}} \text{ tel que } |\hat{t} - t^*| < M \}
```




**F1-score** se define como la media arm√≥nica entre la precisi√≥n y el recall:

```math
\text{F1} = 2 \cdot \frac{\text{Prec} \cdot \text{Rec}}{\text{Prec} + \text{Rec}} \in [0,1]
```

El mejor valor posible para esta m√©trica es 1, indicando una detecci√≥n perfecta, mientras que su peor valor es 0.

#### Hausdorff

Desde un punto de vista formal, esta m√©trica corresponde a la mayor distancia temporal entre un punto de cambio y su correspondiente estimaci√≥n:

```math
\text{Hausdorff}(\mathcal{T}^*,\widehat{\mathcal{T}}) = \max \left\{
\underbrace{
\max_{\hat{t}\in\widehat{\mathcal{T}}} \min_{t^*\in\mathcal{T}^*} |\hat{t}-t^*|
}_{\text{Error m√°ximo de detecci√≥n}},
\underbrace{
\max_{t^*\in\mathcal{T}^*} \min_{\hat{t}\in\widehat{\mathcal{T}}} |\hat{t}-t^*|
}_{\text{Error m√°ximo de omisi√≥n}}
\right\}
```

donde:

 - El primer t√©rmino eval√∫a la m√°xima distancia de cualquier punto detectado al punto real m√°s cercano
 - El segundo t√©rmino mide la m√°xima distancia de cualquier punto real al punto detectado m√°s cercano
\end{itemize}
Este valor representa el peor error cometido por el algoritmo que genera el conjunto de puntos estimados $\(\widehat{\mathcal{T}}\)$, y se expresa en n√∫mero de muestras. Cuando su valor es cero, significa que ambos conjuntos de puntos de cambio coinciden exactamente. Por el contrario, cuanto mayor sea su valor, mayor ser√° la distancia existente entre alg√∫n punto de cambio verdadero en $\(\mathcal{T}^{*}\)$ y el punto estimado m√°s cercano en $\(\widehat{\mathcal{T}}\)$, o viceversa.

#### √çndice de Rand

La m√©trica fundamental √çndice de Rand  cuantifica la precisi√≥n en la detecci√≥n de puntos de cambio. Esta medida estad√≠stica compara la similitud entre la segmentaci√≥n obtenida $\widehat{\mathcal{T}}$ y la segmentaci√≥n de referencia $\mathcal{T}^{*}$, proporcionando una evaluaci√≥n global del rendimiento del algoritmo.

El √çndice de Rand calcula la proporci√≥n de pares de muestras que son:

**Concordantes:**
    
- Pertenecen al mismo segmento en ambas segmentaciones
- Pertenecen a segmentos diferentes en ambas segmentaciones
    
**Discordantes:**

- Asignados al mismo segmento en una segmentaci√≥n y a diferentes en la otra
    



Para formalizar esta idea, se definen las siguientes relaciones para un conjunto de puntos de cambio $\(\mathcal{T}\)$:

$$
\text{SameSeg}(\mathcal{T}) := \{(s,t) \mid 1 \leq s < t \leq T \text{ donde } s \text{ y } t \text{ est√°n en el mismo segmento seg√∫n } \mathcal{T}\}
$$

$$
\text{DiffSeg}(\mathcal{T}) := \{(s,t) \mid 1 \leq s < t \leq T \text{ donde } s \text{ y } t \text{ est√°n en segmentos distintos seg√∫n } \mathcal{T}\}
$$

A partir de estas definiciones, el √çndice de Rand se expresa como:

```math
RI(\mathcal{T}^{*},\widehat{\mathcal{T}}) := \frac{|\text{SameSeg}(\widehat{\mathcal{T}}) \cap \text{SameSeg}(\mathcal{T}^{*})| + |\text{DiffSeg}(\widehat{\mathcal{T}}) \cap \text{DiffSeg}(\mathcal{T}^{*})|}{T(T-1)/2}
```

Este valor se encuentra normalizado en el intervalo entre 0 (cuando no existe ning√∫n acuerdo entre las segmentaciones) y 1 (cuando las segmentaciones son id√©nticas). 



---

[Retour au menu](#Men√∫-de-navegaci√≥n)

---
### Funciones de costo

Esta secci√≥n presenta el primer elemento definitorio de los m√©todos de detecci√≥n de cambios, que son las funciones de costo. En la mayor√≠a de los casos, estas funciones se derivan a partir de un modelo de se√±al. A continuaci√≥n, se agrupa los modelos y sus funciones de costo asociadas en dos categor√≠as: param√©tricas y no param√©tricas.

#### Funci√≥n de Costo L1

Esta funci√≥n de costo detecta cambios en la mediana de una se√±al. En general, es un estimador robusto para detectar desplazamientos en el punto central (ya sea media, mediana o moda) de una distribuci√≥n.

Formalmente, dado un segmento de se√±al `{y_t}_{t ‚àà I}` donde `I` representa el intervalo de an√°lisis, el costo se calcula como:

```math
c(y_I) = ‚àë_{t ‚àà I} ‚Äñy_t - »≥‚Äñ_1
```

donde `»≥` corresponde a la mediana muestral del segmento.

#### Funci√≥n de Costo L2

La funci√≥n CostL2 cuantifica la variabilidad alrededor de la media muestral mediante la norma eucl√≠dea al cuadrado. Para un segmento de se√±al `{y_t}_{t ‚àà I}` donde `I` denota el intervalo de estudio, el costo se define como:

```math
c(y_I) = ‚àë_{t ‚àà I} ‚Äñy_t - »≥‚Äñ_2^2 
```

donde `»≥` corresponde a la media muestral del segmento.

#### Funci√≥n de Costo Normal

Esta funci√≥n de costo permite detectar cambios tanto en la media como en la matriz de covarianza de una secuencia de variables aleatorias gaussianas multivariadas. Formalmente, para un segmento de se√±al `{y_t}_{t ‚àà I}` con `y_t ‚àà ‚Ñù^d`, la funci√≥n de costo se define como:

```math
c(y_I) = |I| ¬∑ log det(Œ£ÃÇ_I + ŒµI_d)
```

donde:
- `Œ£ÃÇ_I = 1/(|I|-1) ‚àë_{t ‚àà I} (y_t - »≥_I)(y_t - »≥_I)^‚ä§` es la matriz de covarianza muestral del segmento
- `»≥_I` es la media emp√≠rica del segmento
- `Œµ > 0` es un t√©rmino de regularizaci√≥n (t√≠picamente `Œµ = 10^{-6}`)
- `I_d` es la matriz identidad de dimensi√≥n `d √ó d`

#### Cambio de media con kernel (CostRbf)

La funci√≥n `CostRbf` opera mediante el mapeo de los datos a un espacio de caracter√≠sticas `H` mediante la funci√≥n `Œ¶(¬∑)`. Para un segmento `{y_t}_{t ‚àà I}` con `y_t ‚àà ‚Ñù^d`, el costo se calcula como:

```math
c(y_I) = ‚àë_{t ‚àà I} ‚ÄñŒ¶(y_t) - ŒºÃÑ‚Äñ_H^2
```

donde `ŒºÃÑ = 1/|I| ‚àë_{t ‚àà I} Œ¶(y_t)` representa la media en el espacio de caracter√≠sticas.

El kernel radial (RBF) implementado tiene la forma:

```math
k(x,y) = exp(-Œ≥‚Äñx - y‚Äñ^2), Œ≥ > 0
```

donde `Œ≥ = 1/mediana({‚Äñy_i - y_j‚Äñ^2}_{i,j})`.

#### Cambio de media con kernel (CostCosine)

La funci√≥n de costo eval√∫a la variabilidad en el espacio de caracter√≠sticas `H` generado por el kernel coseno:

```math
c(y_{a..b}) = ‚àë_{t=a}^{b-1} ‚ÄñŒ¶(y_t) - ŒºÃÑ_{a..b}‚Äñ_H^2
```

donde:
- `Œ¶: ‚Ñù^d ‚Üí H` es el mapeo al espacio de caracter√≠sticas
- `ŒºÃÑ_{a..b} = 1/(b-a) ‚àë_{t=a}^{b-1} Œ¶(y_t)` es la media emp√≠rica en `H`
- El kernel coseno se define como:
  ```math
  k(x,y) = (‚ü®x, y‚ü©)/(‚Äñx‚Äñ_2 ‚Äñy‚Äñ_2) ‚àà [-1,1]
  ```

#### Cambio en modelo lineal (CostLinear)

Consideremos una serie temporal `{y_t}_{t=1}^n` con posibles puntos de cambio en `t_1, t_2, ..., t_k`. El modelo de regresi√≥n por segmentos se define como:

```math
y_t = x_t' Œ¥_j + Œµ_t, t_j ‚â§ t < t_{j+1}
```

donde:
- `y_t ‚àà ‚Ñù`: Variable respuesta
- `x_t ‚àà ‚Ñù^p`: Vector de covariables
- `Œ¥_j ‚àà ‚Ñù^p`: Coeficientes de regresi√≥n para el j-√©simo segmento
- `Œµ_t`: T√©rmino de error con `ùîº[Œµ_t] = 0`

La funci√≥n de costo asociada a un intervalo `I` se define como:

```math
c(y_I) = min_{Œ¥ ‚àà ‚Ñù^p} ‚àë_{t ‚àà I} ‚Äñ y_t - Œ¥' x_t ‚Äñ_2^2
```

#### Cambio lineal continuo (CostCLinear)

Dado un conjunto de knots `{t_k}_{k=1}^K`, el spline lineal continuo `f:‚Ñù‚Üí‚Ñù^d` se define mediante:

- **Comportamiento af√≠n por intervalos**:
  ```math
  f(t) = Œ±_k(t - t_k) + Œ≤_k, Œ±_k,Œ≤_k ‚àà ‚Ñù^d, t ‚àà [t_k,t_{k+1})
  ```
- **Condici√≥n de continuidad**:
  ```math
  lim_{t‚Üít_k^-} f(t) = lim_{t‚Üít_k^+} f(t), ‚àÄk
  ```

La funci√≥n de costo `CostCLinear` se define para `0 < a < b ‚â§ T` como:

```math
c(y_{a,b}) := ‚àë_{t=a}^{b-1} ‚Äñ y_t - y_{a-1} - (t-a+1)/(b-a) (y_{b-1} - y_{a-1}) ‚Äñ^2
```

#### Funci√≥n de costo basada en rangos (CostRank)

La transformaci√≥n de los datos originales `{y_t}_{t=1}^T` a sus rangos `{r_t}_{t=1}^T`:

```math
r_t = text{rango}(y_t text{ en } {y_1,...,y_T})
```

Para un segmento `y_{a..b}`, la funci√≥n de costo:

```math
c_{rank}(a,b) = -(b-a) rÃÑ_{a..b}' Œ£ÃÇ_r^{-1} rÃÑ_{a..b}
```

donde:
- `rÃÑ_{a..b} = 1/(b-a)‚àë_{t=a+1}^b r_t` es la media de rangos
- `Œ£ÃÇ_r` es la matriz de covarianza estimada de los rangos completos

#### Detecci√≥n de cambios con m√©trica Mahalanobis (CostMl)

Dada una matriz semidefinida positiva `M ‚àà ‚Ñù^{d √ó d}`, la pseudom√©trica:

```math
‚Äñx - y‚Äñ_{M}^2 = (x - y)^T M (x - y)
```

Para un segmento `{y_t}_{t ‚àà I}`, la funci√≥n de costo:

```math
c(y_I) = ‚àë_{t ‚àà I} ‚Äñy_t - ŒºÃÑ‚Äñ_M^2
```

donde `ŒºÃÑ = 1/|I|‚àë_{t ‚àà I} y_t` es la media emp√≠rica.

#### Cambio de modelo autorregresivo (CostAR)

Para una serie temporal `{y_t}_{t=1}^n` con posibles puntos de cambio, el modelo AR(p) por segmentos:

```math
y_t = ‚àë_{i=1}^p Œ¥_{j,i} y_{t-i} + œµ_t, t_j ‚â§ t < t_{j+1}
```

La funci√≥n de costo implementada:

```math
c(y_I) = min_{Œ¥ ‚àà ‚Ñù^p} ‚àë_{t ‚àà I} (y_t - Œ¥' z_t)^2
```

con `z_t = [y_{t-1}, ..., y_{t-p}]'`.



---

[Retour au menu](#Men√∫-de-navegaci√≥n)

---


---

En el archivo Generaci√≥n_de_series.ipynb he generado dos series temporales para probar los algoritmos que voy a presentar. Los puntos de cambio en la primera serie temporal son m√°s f√°ciles de detectar,
mientras que en la segunda son m√°s dif√≠ciles de identificar.

En el archivo Varianza_constante_PELT.ipynb Se hace la evaluaci√≥n del Algoritmo PELT con Varianza Constante.
En el archivo Varianza_variable_PELT.ipynb Se hace la evaluaci√≥n del Algoritmo PELT con Varianza Variable.
En el archivo Varianza_constante_ChangeFinder.ipynb Se hace la evaluaci√≥n del Algoritmo ChangeFinder con Varianza Constante.
En el archivo Varianza_variable_ChangeFinder.ipynb Se hace la evaluaci√≥n del Algoritmo ChangeFinder con Varianza Variable.

