

# Detecci√≥n de puntos de cambio en diversas trayectorias

## Men√∫ de navegaci√≥n
1. [Introducci√≥n](#introducci√≥n)
2. [Estado del arte](#estado-del-arte)
3. [Evaluaci√≥n de M√©todos](#evaluaci√≥n-de-M√©todos)
   - [F1-Score](#f1-score)
   - [Distancia de Hausdorff](#distancia-de-hausdorff)
   - [√çndice de Rand](#√≠ndice-de-rand)
4. [Funciones de costo](#funciones-de-costo)
   - [Funci√≥n de Costo L1](#funci√≥n-de-costo-l1)
   - [Funci√≥n de Costo L2](#funci√≥n-de-costo-l2)
   - [Funci√≥n de Costo Normal](#funci√≥n-de-costo-normal)
   - [Cambio de media con kernel (CostRbf)](#cambio-de-media-con-kernel-costrbf)
   - [Cambio de media con kernel (CostCosine)](#cambio-de-media-con-kernel-costcosine)
   - [Cambio en modelo lineal (CostLinear)](#cambio-en-modelo-lineal-costlinear)
   - [Cambio lineal continuo (CostCLinear)](#cambio-lineal-continuo-costclinearcosto)
   - [Funci√≥n de costo basada en rangos (CostRank)](#funci√≥n-de-costo-basada-en-rangos-costrank)
   - [Detecci√≥n de cambios con m√©trica Mahalanobis (CostMl)](#detecci√≥n-de-cambios-con-m√©trica-mahalanobis-costml)
   - [Cambio de modelo autorregresivo (CostAR)](#cambio-de-modelo-autorregresivo-costar)
5. [M√©todos de b√∫squeda de puntos de cambio](#m√©todos-de-b√∫squeda-de-puntos-de-cambio)
   - [Segmentaci√≥n Binaria (BinSeg)](#segmentaci√≥n-binaria-binseg)
   - [Segmentaci√≥n PELT](#segmentaci√≥n-pelt)
   - [Segmentaci√≥n Bottom-Up](#segmentaci√≥n-bottom-up)
   - [Detecci√≥n Window-Based](#detecci√≥n-window-based)
 6.[Aplicaci√≥n de M√©todos](#Aplicaci√≥n-de-M√©todos)  
   - [varianza_constante](#varianza_constante)
   - [varianza_variable](#varianza_variable)



---
### Introducci√≥n

La detecci√≥n de puntos de cambio consiste en identificar momentos en el tiempo donde las propiedades estad√≠sticas de una serie de datos presentan variaciones. Esta t√©cnica es clave en distintos √°mbitos, como las finanzas, el monitoreo ambiental y el control de calidad, ya que entender los cambios en las tendencias de los datos permite mejorar la toma de decisiones y el an√°lisis predictivo. Detectar estos cambios ayuda a los analistas a reaccionar de forma anticipada ante patrones o anomal√≠as que puedan surgir en los datos.
Lo importante de detectar puntos de cambio es que ayuda a entender mejor los datos y tomar decisiones acertadas. Por ejemplo, en el clima, una variaci√≥n repentina en la temperatura o la presi√≥n puede anticipar tormentas o cambios bruscos en el tiempo. En la salud, notar alteraciones en los signos vitales de un paciente puede servir para prevenir problemas graves. En general, identificar estos cambios a tiempo permite reaccionar r√°pido y adaptarse mejor. 
Existen varios m√©todos para detectar puntos de cambio, como las pruebas estad√≠sticas, que comparan los datos antes y despu√©s del cambio, o los algoritmos de aprendizaje autom√°tico, que encuentran patrones en datos m√°s complejos. Tambi√©n se utilizan enfoques bayesianos, que ajustan la detecci√≥n a medida que se recibe nueva informaci√≥n. Cada uno de estos m√©todos tiene sus ventajas dependiendo del tipo de datos y el objetivo del an√°lisis.
Aunque la detecci√≥n de puntos de cambio tiene muchas ventajas, tambi√©n presenta varios desaf√≠os. Uno de los problemas m√°s comunes es elegir el m√©todo adecuado, porque cada t√©cnica puede dar resultados distintos dependiendo del tipo de datos que tengas. Adem√°s, el ruido y los valores at√≠picos pueden complicar el proceso, provocando falsos positivos o incluso dejando cambios sin detectar. Por eso, es importante que los profesionales tomen en cuenta las caracter√≠sticas de sus datos al aplicar estos m√©todos.
Hay varias herramientas y programas que puedes usar para detectar puntos de cambio en los datos. Lenguajes como Python y R tienen bibliotecas especiales para esto. Por ejemplo, en Python puedes usar la biblioteca ruptures, que te permite aplicar diferentes algoritmos para detectar cambios, visualizar los resultados y ajustar los par√°metros seg√∫n lo necesites. En R, existe el paquete changepoint, que tambi√©n facilita este tipo de an√°lisis. 

---

[Retour au menu](#Men√∫-de-navegaci√≥n)

---

### Estado del arte

El problema de los puntos de cambio ha sido y sigue siendo un tema importante en el estudio de trayectorias, ya que permite identificar momentos clave en los que un proceso cambia. Los primeros trabajos sobre este tema fueron de Page en 1954 y 1955, quien present√≥ esquemas de inspecci√≥n continua para detectar cambios en los datos. En 1955, tambi√©n propuso una prueba estad√≠stica para identificar cambios en par√°metros sin conocer el punto exacto. M√°s tarde, Chernoff y Zacks (1964) utilizaron un enfoque bayesiano para estimar la media de una distribuci√≥n normal que cambia con el tiempo. M√°s recientemente, Gichuhi, Franke y Weizsacker (2008) utilizaron redes neuronales para detectar puntos de cambio en datos binarios, lo que abri√≥ nuevas posibilidades con el aprendizaje autom√°tico.
Muchos investigadores tambi√©n han trabajado en los problemas relacionados con los m√∫ltiples puntos de cambio, entre ellos. Bai y Perron (1998) propusieron m√©todos para estimar y probar modelos con m√∫ltiples cambios estructurales. Pan y Chen (2006) usaron un criterio de informaci√≥n modificado para detectar varios puntos de cambio en los datos. Por su parte, Yao (1984) plante√≥ un enfoque bayesiano para estimar funciones escal√≥n ruidosas, mientras que Barry y Hartigan (1992) introdujeron modelos de partici√≥n para manejar los puntos de cambio. Lee (1998) trabaj√≥ en estimar el n√∫mero de puntos de cambio utilizando un enfoque bayesiano. M√°s recientemente, Lavielle (1999) se centr√≥ en detectar m√∫ltiples puntos de cambio en secuencias de variables dependientes, y Lai, Liu y Xing (2005) analizaron modelos autorregresivos con volatilidad constante para tratar los cambios en los par√°metros de los datos.

Uno de los m√°s conocidos es la segmentaci√≥n binaria, que es bastante popular en este tipo de estudios. Es un m√©todo aproximado con una complejidad computacional de \( O(n \log n) \), donde \( n \) es el n√∫mero de puntos de datos. Por otro lado, el algoritmo de vecindad de segmentos (Scott y Knott, 1974) es m√°s preciso, ya que busca de manera exacta en todo el espacio de segmentaci√≥n, pero su costo computacional es mucho mayor, \( O(Qn^2) \), donde \( Q \) es el n√∫mero m√°ximo de puntos de cambio y \( n \) es el n√∫mero de datos. Si el n√∫mero de puntos de cambio crece linealmente con el aumento de los datos, el costo puede llegar a ser \( O(n^3) \), lo que lo hace menos eficiente a medida que se acumulan m√°s datos.

El m√©todo de partici√≥n √≥ptima (Auger y Lawrence, 1989) es otra opci√≥n, que mejora la eficiencia de la vecindad de segmentos, pero sigue sin igualar la rapidez de la segmentaci√≥n binaria. A pesar de esto, es un enfoque exacto, con una complejidad computacional de \( O(n^2) \), y se aplica en un tipo m√°s reducido de problemas. 
El m√©todo PELT (Pruned Exact Linear Time) es una t√©cnica bastante eficiente para detectar m√∫ltiples puntos de cambio en series temporales. En el estudio de Killick, Eckley y Jonathan (2011), este m√©todo se aplic√≥ a series de datos oce√°nicos y mostr√≥ ser muy r√°pido en comparaci√≥n con otras t√©cnicas, ya que puede manejar grandes vol√∫menes de datos en un tiempo relativamente corto. La clave de su eficiencia est√° en que PELT realiza una poda en el proceso de b√∫squeda, lo que significa que no tiene que revisar todas las posibles divisiones de los datos, sino solo las m√°s relevantes, haciendo que funcione en un tiempo O(n). Por otro lado, en el trabajo de Madon y Hingrat (2014), PELT se utiliz√≥ en combinaci√≥n con un √°rbol de clasificaci√≥n para analizar el comportamiento de los animales. Este enfoque ayud√≥ a detectar cambios significativos en sus movimientos de manera r√°pida y precisa, lo que es especialmente √∫til cuando se tiene que manejar una gran cantidad de datos sobre los patrones de movimiento de los animales.

---

[Retour au menu](#Men√∫-de-navegaci√≥n)

---

#### Evaluaci√≥n


Los m√©todos para detectar puntos de cambio se pueden evaluar de dos formas: una es demostrando ciertas propiedades matem√°ticas de los algoritmos y la otra es hacerlo de forma emp√≠rica, calculando distintas m√©tricas.

En lo que sigue, al conjunto de los puntos de cambio verdaderos lo denoto como $`\mathcal{T}^* = \{ t_1^*, \ldots, t_K^* \}`$, y al conjunto de los puntos de cambio estimados lo denoto como $`\widehat{\mathcal{T}} = \{\hat{t}_1, \ldots, \hat{t}_{\hat{K}} \}`$.
#### F1-Score

La m√©trica F1-Score emerge como indicador robusto para evaluar el rendimiento en esta tarea. Su c√°lculo se basa en dos componentes esenciales:

**Precisi√≥n (Prec)**: Mide la fiabilidad de las detecciones
```math
\text{Prec} = \frac{\text{Detecciones correctas}}{\text{Total de detecciones}} = \frac{|\text{Tp}|}{\hat{K}}
```

**Exhaustividad (Rec)**: Eval√∫a la capacidad de descubrimiento
```math
\text{Rec} = \frac{\text{Detecciones correctas}}{\text{Total de puntos reales}} = \frac{|\text{Tp}|}{K^*}
```

 Considero que una detecci√≥n es v√°lida cuando existe coincidencia dentro de un margen $M$ muestral:
```math
\text{Tp} = \big\{ t^* \in \mathcal{T}^* \mid \exists\, \hat{t} \in \widehat{\mathcal{T}} \,:\, |\hat{t} - t^*| < M \big\}
```

**F1-score** se define como la media arm√≥nica entre la precisi√≥n y el recall:

```math
\text{F1} = 2 \cdot \frac{\text{Prec} \cdot \text{Rec}}{\text{Prec} + \text{Rec}} \in [0,1]
```

El mejor valor posible para esta m√©trica es 1, indicando una detecci√≥n perfecta, mientras que su peor valor es 0.
#### Hausdorff

Desde un punto de vista formal, esta m√©trica corresponde a la mayor distancia temporal entre un punto de cambio y su correspondiente estimaci√≥n:

```math
\text{Hausdorff}(\mathcal{T}^*,\widehat{\mathcal{T}}) = \max \left\{
\underbrace{
\max_{\hat{t}\in\widehat{\mathcal{T}}} \min_{t^*\in\mathcal{T}^*} |\hat{t}-t^*|
}_{\text{Error m√°ximo de detecci√≥n}},
\underbrace{
\max_{t^*\in\mathcal{T}^*} \min_{\hat{t}\in\widehat{\mathcal{T}}} |\hat{t}-t^*|
}_{\text{Error m√°ximo de omisi√≥n}}
\right\}
```

donde:

- El primer t√©rmino eval√∫a la m√°xima distancia de cualquier punto detectado al punto real m√°s cercano
- El segundo t√©rmino mide la m√°xima distancia de cualquier punto real al punto detectado m√°s cercano

Este valor representa el peor error cometido por el algoritmo que genera el conjunto de puntos estimados $`\widehat{\mathcal{T}}`$, y se expresa en n√∫mero de muestras. Cuando su valor es cero, significa que ambos conjuntos de puntos de cambio coinciden exactamente. Por el contrario, cuanto mayor sea su valor, mayor ser√° la distancia existente entre alg√∫n punto de cambio verdadero en $`\mathcal{T}^{*}`$ y el punto estimado m√°s cercano en $`\widehat{\mathcal{T}}`$, o viceversa.

\subsection{ √çndice de Rand}

La m√©trica fundamental √çndice de Rand  cuantifica la precisi√≥n en la detecci√≥n de puntos de cambio. Esta medida estad√≠stica compara la similitud entre la segmentaci√≥n obtenida $`\widehat{\mathcal{T}}`$ y la segmentaci√≥n de referencia $`\mathcal{T}^{*}`$, proporcionando una evaluaci√≥n global del rendimiento del algoritmo.

El √çndice de Rand calcula la proporci√≥n de pares de muestras que son:

- **Concordantes**: 
 
- Pertenecen al mismo segmento en ambas segmentaciones
-Pertenecen a segmentos diferentes en ambas segmentaciones

 **Discordantes**:
    
- Asignados al mismo segmento en una segmentaci√≥n y a diferentes en la otra
   
Para formalizar esta idea, se definen las siguientes relaciones para un conjunto de puntos de cambio $`\mathcal{T}`$:

**SameSeg(T)** :  
`{(s,t) | 1 ‚â§ s < t ‚â§ T` o√π `s` et `t` sont dans le m√™me segment selon `T`}

**DiffSeg(T)** :  
`{(s,t) | 1 ‚â§ s < t ‚â§ T` o√π `s` et `t` sont dans des segments diff√©rents selon `T`}

A partir de estas definiciones, el √çndice de Rand se expresa como:

```math
RI(\mathcal{T}^{*},\widehat{\mathcal{T}}) := \frac{|\text{SameSeg}(\widehat{\mathcal{T}}) \cap \text{SameSeg}(\mathcal{T}^{*})| + |\text{DiffSeg}(\widehat{\mathcal{T}}) \cap \text{DiffSeg}(\mathcal{T}^{*})|}{T(T-1)/2}
```

Este valor se encuentra normalizado en el intervalo entre 0 (cuando no existe ning√∫n acuerdo entre las segmentaciones) y 1 (cuando las segmentaciones son id√©nticas). 
---

[Retour au menu](#Men√∫-de-navegaci√≥n)

---
### funciones de costo

Esta secci√≥n presenta el primer elemento definitorio de los m√©todos de detecci√≥n de cambios, que son las funciones de costo. En la mayor√≠a de los casos, estas funciones se derivan a partir de un modelo de se√±al. A continuaci√≥n, se agrupa los modelos y sus funciones de costo asociadas en dos categor√≠as: param√©tricas y no param√©tricas.

### Funci√≥n de Costo L1

Esta funci√≥n de costo detecta cambios en la mediana de una se√±al. En general, es un estimador robusto para detectar desplazamientos en el punto central (ya sea media, mediana o moda) de una distribuci√≥n .
Formalmente, dado un segmento de se√±al $`\{y_t\}_{t \in I}`$ donde $I$ representa el intervalo de an√°lisis, el costo se calcula como:

```math
c(y_I) = \sum_{t \in I} \|y_t - \tilde{y}\|_1
```

donde $`\tilde{y}`$ corresponde a la mediana muestral del segmento.

#### Funci√≥n de Costo L2
La funci√≥n CostL2 cuantifica la variabilidad alrededor de la media muestral mediante la norma eucl√≠dea al cuadrado. Para un segmento de se√±al $`\{y_t\}_{t \in I}`$ donde $I$ denota el intervalo de estudio, el costo se define como:

```math
c(y_I) = \sum_{t \in I} \|y_t - \bar{y}\|_2^2 
```

donde $`\tilde{y}`$ corresponde a la mediana muestral del segmento.

#### Funci√≥n de Costo Normal
Esta funci√≥n de costo permite detectar cambios tanto en la media como en la matriz de covarianza de una secuencia de variables aleatorias gaussianas multivariadas. Formalmente, para un segmento de se√±al $`\{y_t\}_{t \in I}`$ con $`y_t \in \mathbb{R}^d`$, la funci√≥n de costo se define como:

```math
c(y_I) = |I| \cdot \log \det(\widehat{\Sigma}_I + \epsilon I_d)
```

donde:
```math
 `\widehat{\Sigma}_I = \frac{1}{|I|-1} \sum_{t \in I} (y_t - \bar{y}_I)(y_t - \bar{y}_I)^\top`
```
es la matriz de covarianza muestral del segmento.
  - $`\bar{y}_I`$ es la media emp√≠rica del segmento.
  - $`\epsilon > 0`$ es un t√©rmino de regularizaci√≥n (t√≠picamente $`\epsilon = 10^{-6}`$) que se a√±ade para evitar problemas num√©ricos en matrices mal condicionadas.
 - $`I_d`$ es la matriz identidad de dimensi√≥n $`d \times d`$.

#### Cambio de media con kernel (CostRbf)

La funci√≥n **CostRbf** opera mediante el mapeo de los datos a un espacio de caracter√≠sticas $`\mathcal{H}`$ mediante la funci√≥n $`\Phi(\cdot)`$, donde se analizan las propiedades estad√≠sticas. Para un segmento $`\{y_t\}_{t \in I}`$ con $`y_t \in \mathbb{R}^d`$, el costo se calcula como:

```math
c(y_I) = \sum_{t \in I} \|\Phi(y_t) - \bar{\mu}\|_{\mathcal{H}}^2
```

donde $`\bar{\mu} = \frac{1}{|I|}\sum_{t \in I} \Phi(y_t)`$ representa la media en el espacio de caracter√≠sticas.
 
El kernel radial (RBF) implementado tiene la forma:

```math
k(x,y) = \exp\left(-\gamma \|x - y\|^2\right), \quad \gamma > 0
```

donde $`\gamma = 1/\text{mediana}(\{\|y_i - y_j\|^2\}_{i,j})`$.

#### Cambio de media con kernel (CostCosine)

La funci√≥n de costo eval√∫a la variabilidad en el espacio de caracter√≠sticas $`\mathcal{H}`$ generado por el kernel coseno:

```math
c(y_{a..b}) = \sum_{t=a}^{b-1} \|\Phi(y_t) - \bar{\mu}_{a..b}\|_{\mathcal{H}}^2
```

donde:

- $`\Phi: \mathbb{R}^d \rightarrow \mathcal{H}`$ es el mapeo al espacio de caracter√≠sticas
- $`\bar{\mu}_{a..b} = \frac{1}{b-a}\sum_{t=a}^{b-1} \Phi(y_t)`$ es la media emp√≠rica en $`\mathcal{H}`$
- El kernel coseno se define como:

```math
k(x,y) = \frac{\langle x, y \rangle}{\|x\|_2 \|y\|_2} \in [-1,1]
```


donde $`\langle \cdot \mid \cdot \rangle$ y $\|\cdot\|`$ corresponden al producto escalar y la norma euclidiana respectivamente. Dicho de otro modo, equivale al producto punto normalizado en norma L2 de los vectores.

#### Cambio en modelo lineal (CostLinear)
Consideremos una serie temporal $`\{y_t\}_{t=1}^n`$ con posibles puntos de cambio en $`t_1, t_2, \ldots, t_k`$. El modelo de regresi√≥n por segmentos se define como:

```math
y_t = x_t' \delta_j + \varepsilon_t, \quad t_j \leq t < t_{j+1}
```

donde:

- $`y_t \in \mathbb{R}`$: Variable respuesta
- $`x_t \in \mathbb{R}^p`$: Vector de covariables
- $`\delta_j \in \mathbb{R}^p`$: Coeficientes de regresi√≥n para el $j$-√©simo segmento
- $`\varepsilon_t`$: T√©rmino de error con $`\mathbb{E}[\varepsilon_t] = 0`$


Las estimaciones por m√≠nimos cuadrados de las fechas de ruptura se obtienen minimizando la suma de los residuos al cuadrado. Formalmente, la funci√≥n de costo asociada a un intervalo $I$ se define como:

```math
c(y_I) = \min_{\delta \in \mathbb{R}^p} \sum_{t \in I} \| y_t - \delta' x_t \|_2^2
```
#### Cambio lineal continuo (CostCLinear)
Dado un conjunto de knots  $`\{t_k\}_{k=1}^K`$, el spline lineal continuo $`f:\mathbb{R}\rightarrow\mathbb{R}^d`$ se define mediante:


- **Comportamiento af√≠n por intervalos**:
    ```math
    f(t) = \alpha_k(t - t_k) + \beta_k, \quad \alpha_k,\beta_k \in \mathbb{R}^d, \quad t \in [t_k,t_{k+1})
   ```
    
- **Condici√≥n de continuidad**:
    ```math
    \lim_{t\to t_k^-} f(t) = \lim_{t\to t_k^+} f(t), \quad \forall k
    ```

La funci√≥n de costo **CostCLinear** mide el error al aproximar la se√±al con una spline lineal. Formalmente, se define para $`0 < a < b \leq T`$ como:

```math
c(y_{a,b}) := \sum_{t=a}^{b-1} \left\| y_t - y_{a-1} - \frac{t-a+1}{b-a} (y_{b-1} - y_{a-1}) \right\|^2
```

y se toma $`c(y_{0,b}) = c(y_{1,b})`$.

#### Funci√≥n de costo basada en rangos (CostRank)
La clave de este m√©todo reside en la transformaci√≥n de los datos originales $`\{y_t\}_{t=1}^T`$ a sus rangos $`\{r_t\}_{t=1}^T`$, donde:

```math
r_t = \text{rango}(y_t \text{ en } \{y_1,\ldots,y_T\})
```

Para un segmento $`y_{a..b}`$, la funci√≥n de costo se define como:

```math
c_{\text{rank}}(a,b) = -(b-a) \bar{r}_{a..b}^\prime \widehat{\Sigma}_r^{-1} \bar{r}_{a..b}
```

donde:

- $`\bar{r}_{a..b} = \frac{1}{b-a}\sum_{t=a+1}^b r_t`$ es la media de rangos en el segmento
- $`\widehat{\Sigma}_r`$ es la matriz de covarianza estimada de los rangos completos

#### Detecci√≥n de cambios con una m√©trica de tipo Mahalanobis (CostMl)

Dada una matriz semidefinida positiva $`M \in \mathbb{R}^{d \times d}`$, definimos la pseudom√©trica:

```math
\|x - y\|_{M}^2 = (x - y)^T M (x - y)
```

Para un segmento de se√±al $`\{y_t\}_{t \in I}`$, la funci√≥n de costo se calcula como:

```math
c(y_I) = \sum_{t \in I} \|y_t - \bar{\mu}\|_M^2
```

donde $`\bar{\mu} = \frac{1}{|I|}\sum_{t \in I} y_t`$ es la media emp√≠rica del segmento.

#### Cambio de modelo autorregresivo (CostAR)

Considerando una serie temporal $`\{y_t\}_{t=1}^n`$ con posibles puntos de cambio en $`\{t_j\}_{j=1}^k`$, el modelo $`AR``(p)`$ por segmentos se define como:

```math
y_t = \sum_{i=1}^p \delta_{j,i} y_{t-i} + \epsilon_t, \quad t_j \leq t < t_{j+1}
```

donde:

- $`p`$: Orden del modelo (seleccionado mediante AIC en mi implementaci√≥n)
- $`\delta_j \in \mathbb{R}^p`$: Coeficientes AR para el `j`-√©simo segmento
- $`\epsilon_t`$: Innovaciones con $`\mathbb{E}[\epsilon_t] = 0`, `\text{Var}(\epsilon_t) = \sigma^2`$

La funci√≥n de costo implementada minimiza la suma de residuos al cuadrado:

```math
c(y_I) = \min_{\delta \in \mathbb{R}^p} \sum_{t \in I} (y_t - \delta' z_t)^2
```

con $`z_t = [y_{t-1}, \ldots, y_{t-p}]'`$.
---

[Retour au menu](#Men√∫-de-navegaci√≥n)

---
### M√©todos de b√∫squeda de puntos de cambio

Esta secci√≥n presenta el segundo elemento definitorio de los m√©todos de detecci√≥n de cambios, concretamente el **m√©todo de b√∫squeda**. Los algoritmos de b√∫squeda determinan c√≥mo se exploran las posibles configuraciones de puntos de cambio en la serie temporal, afectando tanto a la precisi√≥n como a la eficiencia computacional del m√©todo.
#### Segmentaci√≥n Binaria (BinSeg)

El algoritmo de **Segmentaci√≥n Binaria (BinSeg)** es un m√©todo iterativo para detectar puntos de cambio en series temporales que opera mediante una estrategia greedy. En cada iteraci√≥n, el algoritmo identifica el punto de cambio √≥ptimo que minimiza la suma de costos de los segmentos adyacentes:

$tÃÇ‚ÅΩ·µè‚Åæ = argmin_[a<t<b] c(y_[a..t]) + c(y_[t..b])$

donde $`c(\cdot)`$ representa t√≠picamente el error cuadr√°tico medio. A pesar de su eficiencia computacional         ($`\mathcal{O}(n \log n)`$), el enfoque greedy implica que cada punto de cambio se estima condicionado a los cambios anteriores, lo que puede afectar la optimalidad global. 
## Algoritmo BinSeg (Binary Segmentation)

**Input**: 
- Se√±al $`{y_t}_{t=1}^T`$ (serie temporal)
- Funci√≥n de costo $`c(¬∑)`$ (ej: L1, L2, normal)
- Criterio de parada (ej: n√∫mero m√°ximo de cambios o umbral de ganancia)

**Output**: 
- Conjunto $`L`$ de puntos de cambio estimados (√≠ndices)

```python
1: Initialize L = ‚àÖ  
2: repeat
3:   k = |L|  
4:   t‚ÇÄ = 0, t_{k+1} = T  
5:   if k > 0 then
6:     Sort L = {t‚ÇÅ, ..., t_k} ascendentemente  
7:   end if
8:   Initialize array G of length k+1  
9:   for i = 0 to k do
10:    
11:    G[i] = c(y_{t_i..t_{i+1}}) - min_{t_i < t < t_{i+1}} [c(y_{t_i..t}) + c(y_{t..t_{i+1}})]
12:  end for
13:  √Æ = argmax_i G[i]  
14:  
15:  tÃÇ = argmin_{t_√Æ < t < t_{√Æ+1}} [c(y_{t_√Æ..t}) + c(y_{t..t_{√Æ+1}})]
16:  L = L ‚à™ {tÃÇ} 
17: until stopping criterion is met  
18: return sorted(L)  
```




El algoritmo termina cuando se alcanza un n√∫mero m√°ximo de cambios o cuando la m√°xima ganancia $G[i]$ est√° por debajo de un umbral predefinido. Esta aproximaci√≥n balancea eficiencia computacional con capacidad de detecci√≥n, siendo particularmente √∫til cuando el n√∫mero de segmentos es desconocido a priori.


#### Segmentaci√≥n PELT (Pruned Exact Linear Time)
El m√©todo PELT es un algoritmo de detecci√≥n exacta de puntos de cambio que combina optimalidad global con eficiencia computacional mediante t√©cnicas de poda din√°mica. A diferencia de m√©todos aproximados como BinSeg, PELT garantiza encontrar la partici√≥n √≥ptima de la serie temporal minimizando:

```math
\sum_{i=1}^{m+1} c(y_{\tau_{i-1}:\tau_i}) + \beta m
```

donde $c(\cdot)$ es la funci√≥n de costo, $\beta$ el par√°metro de penalizaci√≥n y $m$ el n√∫mero de cambios. La clave del algoritmo reside en su capacidad para descartar particiones sub√≥ptimas manteniendo √∫nicamente las soluciones relevantes.

## Algoritmo PELT (Pruned Exact Linear Time)

**Input**:
- Se√±al $`{y_t}_{t=1}^T`$
- Funci√≥n de costo $`c(¬∑)`$
- Par√°metro de penalizaci√≥n $`Œ≤`$

**Output**:
- Conjunto $`L[T]`$ de puntos de cambio estimados

```python
1: Initialize Z[0] = -Œ≤ 
2: Initialize L[0] = ‚àÖ   
3: Initialize œá = {0}   
4: for t = 1 to T do
5:  
6:   tÃÇ = argmin_{s ‚àà œá} [Z[s] + c(y_{s:t}) + Œ≤]
7:   
8:   
9:   Z[t] = Z[tÃÇ] + c(y_{tÃÇ:t}) + Œ≤
10:  
11:  
12:  L[t] = L[tÃÇ] ‚à™ {tÃÇ}
13:  
14:  
15:  œá = {s ‚àà œá | Z[s] + c(y_{s:t}) ‚â§ Z[t]} ‚à™ {t}
16: end for
17: return L[T]
```


El algoritmo mantiene un conjunto activo $\chi$ de √≠ndices candidatos, actualizando en cada iteraci√≥n tanto los costos acumulados $Z[t]$ como la lista de cambios $L[t]$. La regla de poda (l√≠nea 9) asegura que solo se conserven las particiones que potencialmente pueden llevar a la soluci√≥n √≥ptima global, reduciendo as√≠ la complejidad computacional sin sacrificar precisi√≥n.
#### Segmentaci√≥n Bottom-Up
El m√©todo Bottom-Up es un enfoque no greedy para la detecci√≥n de puntos de cambio que opera mediante fusi√≥n progresiva de segmentos. A diferencia de m√©todos como BinSeg que dividen la se√±al, este algoritmo sigue una estrategia de unificaci√≥n: comienza con una partici√≥n inicial fina (definida por un par√°metro de grilla $\delta$) y fusiona iterativamente los pares de segmentos m√°s similares hasta satisfacer un criterio de parada. Matem√°ticamente, el proceso optimiza:

```math
\min_{L} \sum_{i=0}^{k} c(y_{t_i:t_{i+1}})
```

donde $L = \{t_1, \ldots, t_k\}$ son los puntos de cambio y $c(\cdot)$ es la funci√≥n de costo. La clave del m√©todo reside en su m√©trica de fusi√≥n:

```math
G[i] = c(y_{t_{i-1}:t_{i+1}}) - \left[c(y_{t_{i-1}:t_i}) + c(y_{t_i:t_{i+1}})\right]
```
que cuantifica la ganancia al fusionar dos segmentos adyacentes.

## Algoritmo Bottom-Up

**Input**:
- Se√±al $`{y_t}_{t=1}^T`$
- Funci√≥n de costo $`c(¬∑)`$
- Tama√±o de grilla $`Œ¥ > 2`$
- Criterio de parada

**Output**:
- Conjunto $`L`$ de puntos de cambio estimados

```python
1: Initialize L = {Œ¥, 2Œ¥, ..., (‚åäT/Œ¥‚åã-1)Œ¥}  
2: repeat
3:   k = |L|  
4:   Sort L = {t‚ÇÅ, ..., t_k} ascendentemente
5:   Initialize array G of length k-1  
6:   for i = 1 to k-1 do
7:    
8:     G[i-1] = c(y_{t_{i-1}:t_{i+1}}) - [c(y_{t_{i-1}:t_i}) + c(y_{t_i:t_{i+1}})]
9:   end for
10:  √Æ = argmin_i G[i]  
11:  L = L \ {t_{√Æ+1}}  
12: until stopping criterion is met
13: return L
```
#### Detecci√≥n Window-Based

El m√©todo **Window-Based** es un algoritmo eficiente para detectar puntos de cambio mediante el an√°lisis de discrepancia entre **segmentos adyacentes de tama√±o \(w\)**. Utiliza dos fragmentos de la se√±al $\(\{y_t\}_{t=1}^T\)$ que se deslizan a lo largo de ella, comparando sus propiedades estad√≠sticas mediante una medida de discrepancia derivada de la funci√≥n de costo $\(c(\cdot)\)$.

```math
d(y_{u..w}, y_{v..w}) = c(y_{u..w}) - [c(y_{v..w}) + c(y_{u..v})]
```

donde $u < v < w$ son √≠ndices temporales. La curva de discrepancia se define para cada punto $t$ como:

```math
Z[t] = c(y_{t-w..t+w}) - [c(y_{t-w..t}) + c(y_{t..t+w})]
```

Los picos en esta curva indican potenciales puntos de cambio, detectados mediante un procedimiento de b√∫squeda de m√°ximos locales (PKSearch).

## Algoritmo Window-Based

**Input**:
- Se√±al $`{y_t}_{t=1}^T`$
- Funci√≥n de costo $`c(¬∑)`$
- Ancho de media ventana $`w`$
- Procedimiento $`PKSearch`$ (detecci√≥n de picos)

**Output**:
- Conjunto $`L`$ de puntos de cambio estimados

```python
1: Initialize Z = [0,...,0]  
2: for t = w to T-w do
3:   p = (t-w)..t      
4:   q = t..(t+w)      
5:   r = (t-w)..(t+w)  
6:   Z[t] = c(y_r) - [c(y_p) + c(y_q)]  
7: end for
8: L = PKSearch(Z)     
```

---

[Retour au menu](#Men√∫-de-navegaci√≥n)

---

### Aplicaci√≥n de M√©todos

El proyecto se organiza en dos carpetas principales dentro de `notebooks/`:

#### üìÅ `varianza_constante/`  
Contiene los notebooks que trabajan con una serie temporal de **varianza constante**:

- `Generacion_varianza_constante.ipynb` ‚Üí Generaci√≥n de la serie.
- `Deteccion_PELT.ipynb` ‚Üí Aplicaci√≥n del algoritmo **PELT**.
- `Deteccion_ChangeFinder.ipynb` ‚Üí Aplicaci√≥n del algoritmo **ChangeFinder**.
- `Deteccion_BinSeg.ipynb` ‚Üí Aplicaci√≥n del m√©todo **Binary Segmentation**.
- `Deteccion_WindowBased.ipynb` ‚Üí Aplicaci√≥n del m√©todo **Window-Based**.
- `Deteccion_BottomUp.ipynb` ‚Üí Aplicaci√≥n del m√©todo **Bottom-Up**.


---

#### üìÅ `varianza_variable/`  
Contiene los notebooks que trabajan con una serie temporal de **varianza variable**, donde los puntos de cambio son m√°s dif√≠ciles de identificar:

- `Generacion_varianza_variable.ipynb` ‚Üí Generaci√≥n de la serie.
- `Deteccion_PELT.ipynb` ‚Üí Evaluaci√≥n del algoritmo **PELT**.
- `Deteccion_ChangeFinder.ipynb` ‚Üí Evaluaci√≥n del algoritmo **ChangeFinder**.
- `Deteccion_BinSeg.ipynb` ‚Üí Aplicaci√≥n del m√©todo **Binary Segmentation**.
- `Deteccion_WindowBased.ipynb` ‚Üí Aplicaci√≥n del m√©todo **Window-Based**.
- `Deteccion_BottomUp.ipynb` ‚Üí Aplicaci√≥n del m√©todo **Bottom-Up**.

---